{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "299cc103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Legion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Legion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Legion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your first document text: Large Language Models can be trained for a number of purposes. In this assignment, candidates are either required to (CASE 1) implement an algorithm to generate summarization of an input text following the style of another text given as input in the context window. The implementation is to be either in Python, while using NLTK, or in Java while using OpenNLP.  A text is given \"as it is\" when the length is below the context window limit (128 Mb or 4000 tokens, or other depending on the chosen LLM). When it exceeds the window, then the algorithm should provide a summary with an hierarchical approach:  The pipeline will be   * Measure the length of the two documents; * Compute the target lengths in a proportional way with respect to the length of the documents; * Slice the second document from start to a point within the context window; * Summarize the slice with no request for size of the target; * Repeat the previous two steps until the end of the document; * Collate the summaries above;  * Repeat the shrinking activities until the summary size is within the context window; * Save the document; * Repeat the summarization for the second document; * Generate the query.  The assignment does not require the evaluation of the performances.\n",
      "Enter your second document text: In the realm of natural language processing, Large Language Models (LLMs) play a pivotal role. GPT-3.5, among others, exemplifies the prowess of these models in comprehending and producing text akin to human language. Extensively trained on diverse datasets, LLMs excel at grasping intricate linguistic subtleties. Their utilization spans a spectrum of applications, encompassing chatbot interactions, language translation, code creation, and content summarization. The evolving capabilities of LLMs herald exciting prospects in the realm of artificial intelligence.Python repository leveraging NLTK and spaCy for text summarization and query generation. Input text and style reference trigger tokenization, stemming, and frequency-based summarization. The summary is analyzed for named entities and sentiment, forming a concise query. An adaptable solution for text summarization and query generation\n",
      "\n",
      "Final Summary saved to 'final_summary.txt'.\n",
      " realm natur languag process larg languag model llm play pivot role among exemplifi prowess model comprehend produc text akin human languag exten traindiv datasetllm excel grasp intric linguit subtleti heir utilspan spectrum applic encompass chatbot intractionlanguagtransl codecr contentsummarizationevolvcapabilitif llmheraldexcit prospectsrealmrtificialintelligenthonrepoitorileverag nltkspacitextsummarizatind quergener nputextndtylereferenceriggerokeniziontemmgndfquencyedumrizatithesummarnalyzefornaentitiesndentintformgconcequernadaptablelutifexsummarizionquergenerati\n",
      "\n",
      "Generated Query: realm languag process larg languag model llm play pivot role exemplifi prowess model produc text akin human languag exten traindiv datasetllm grasp intric linguit heir utilspan spectrum applic encompass chatbot intractionlanguagtransl codecr contentsummarizationevolvcapabilitif llmheraldexcit prospectsrealmrtificialintelligenthonrepoitorileverag quergener nputextndtylereferenceriggerokeniziontemmgndfquencyedumrizatithesummarnalyzefornaentitiesndentintformgconcequernadaptablelutifexsummarizionquergenerati\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def generate_query(summary):\n",
    "    tokens = word_tokenize(summary)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    query_terms = [word for word, pos in pos_tags if pos.startswith('NN') or pos.startswith('JJ')]\n",
    "    query = \" \".join(query_terms)\n",
    "\n",
    "    return query\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    return [\" \".join([ps.stem(word.lower()) for word in word_tokenize(sentence) if word.isalnum() and word.lower() not in stop_words]) for sentence in sent_tokenize(text)]\n",
    "\n",
    "def generate_summary(document, target_length):\n",
    "    sentences = preprocess_text(document)\n",
    "    summary = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        words_count = len(sentence.split())\n",
    "        if current_length + words_count <= target_length:\n",
    "            summary.append(sentence)\n",
    "            current_length += words_count\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \" \".join(summary)\n",
    "\n",
    "\n",
    "def hierarchical_summarization_with_query(doc1, doc2):\n",
    "    total_length = len(doc1.split()) + len(doc2.split())\n",
    "    target_length_doc1 = int(len(doc1.split()) / total_length * 128)\n",
    "    target_length_doc2 = 128 - target_length_doc1\n",
    "    collated_summary = \"\"\n",
    "\n",
    "    while len(collated_summary.split()) < target_length_doc2 and doc2:\n",
    "        slice_text = doc2[:target_length_doc2 - len(collated_summary.split())]\n",
    "        summary = generate_summary(slice_text, target_length_doc2)\n",
    "        collated_summary += summary\n",
    "        doc2 = doc2[len(slice_text):]\n",
    "\n",
    "    final_summary = generate_summary(collated_summary, 128)\n",
    "    with open(\"final_summary.txt\", \"w\") as file:\n",
    "        file.write(final_summary)\n",
    "    generated_query = generate_query(final_summary)\n",
    "    return final_summary, generated_query\n",
    "\n",
    "\n",
    "document1 = input(\"Enter your first document text: \")\n",
    "document2 = input(\"Enter your second document text: \")\n",
    "\n",
    "result_summary, generated_query = hierarchical_summarization_with_query(document1, document2)\n",
    "\n",
    "print(\"\\nFinal Summary saved to 'final_summary.txt'.\\n\", result_summary)\n",
    "print(\"\\nGenerated Query:\", generated_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade39255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
